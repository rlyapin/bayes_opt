{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the GP Latent Variable Model as introduced in (Titsias and Lawrence, Bayesian Gaussian Process Latent Variable Model, 2010, http://proceedings.mlr.press/v9/titsias10a/titsias10a.pdf) paper.\n",
    "\n",
    "Main difference compared to previous notebook lies in switching to tensorflow to perform model training / optimizing variational bound on the log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a 2D signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity I generate the signal over 2D domain using ARD kernel (it is used as a default kernel for variational distribution in the base paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGMA_OBS = 0.1\n",
    "ARD_KERNEL_SIGMA = 1\n",
    "ARD_WEIGHT_0 = 2\n",
    "ARD_WEIGHT_1 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.random.randn(1000)\n",
    "x1 = np.random.randn(1000)\n",
    "\n",
    "x0_matrix = np.tile(x0, len(x0)).reshape((len(x0), len(x0)))\n",
    "x0_matrixT = x0_matrix.transpose()\n",
    "x1_matrix = np.tile(x1, len(x1)).reshape((len(x1), len(x1)))\n",
    "x1_matrixT = x1_matrix.transpose()\n",
    "\n",
    "y_K = np.exp(- ARD_WEIGHT_0 * (x0_matrix - x0_matrixT) ** 2 / 2 \n",
    "             - ARD_WEIGHT_1 * (x1_matrix - x1_matrixT) ** 2 / 2 ) * ARD_KERNEL_SIGMA ** 2\n",
    "while True:\n",
    "    try:\n",
    "        np.linalg.cholesky(y_K)\n",
    "        break\n",
    "    except:\n",
    "        y_K += 0.0001 * np.eye(len(x0))\n",
    "\n",
    "y = np.dot(np.linalg.cholesky(y_K), np.random.randn(len(x0)).reshape(-1, 1)) + SIGMA_OBS * np.random.randn(len(x0)).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "surf = ax.plot_trisurf(list(x0), list(x1), list(y.reshape(-1)), cmap=cm.jet, linewidth=0.1)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.title(\"Observed signal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Latent Variable Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OBS = 1000\n",
    "N_LATENT_VARIABLES = 25\n",
    "N_LATENT_DIMS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_variational = tf.Variable(tf.random_normal([N_OBS, N_LATENT_DIMS]))\n",
    "var_variational = tf.Variable(tf.random_normal([N_OBS, N_LATENT_DIMS])) ** 2\n",
    "latent_variables = tf.Variable(tf.random_normal([N_LATENT_VARIABLES, N_LATENT_DIMS]))\n",
    "y_obs = tf.constant(y, shape=(N_OBS, 1), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = tf.Variable(1.0)\n",
    "ard_sigma = tf.Variable(1.0)\n",
    "ard_weights = tf.Variable([1.0, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating main auxilliary variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K table has [N_LATENT_VARIABLES, N_LATENT_VARIABLES] size\n",
    "# During the construction of it I need to reduce over N_LATENT_VARIABLES\n",
    "# Expanding all variables to meet [N_LATENT_VARIABLES, N_LATENT_DIMS, N_LATENT_VARIABLES] format\n",
    "K_left_latent = tf.tile(tf.expand_dims(latent_variables, axis=-1), [1, 1, N_LATENT_VARIABLES])\n",
    "K_right_latent = tf.tile(tf.expand_dims(tf.transpose(latent_variables), axis=0), [N_LATENT_VARIABLES, 1, 1])\n",
    "K_expanded_weights = tf.reshape(ard_weights, shape=[1, N_LATENT_DIMS, 1])\n",
    "\n",
    "K_diff = tf.reduce_sum(K_expanded_weights * (K_left_latent - K_right_latent) ** 2, axis=1)\n",
    "K = ard_sigma ** 2 * tf.exp(-0.5 * K_diff)\n",
    "\n",
    "print K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining psi_0 (which is just a constant)\n",
    "psi_0 = N_OBS * ard_sigma ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The psi_1 table has [N_OBS, N_LATENT_VARIABLES] size\n",
    "# Expanding all variables to meet [N_OBS, N_LATENT_VARIABLES, N_LATENT_DIMS] format\n",
    "psi_1_tiled_mu = tf.tile(tf.expand_dims(mu_variational, axis=1), [1, N_LATENT_VARIABLES, 1])\n",
    "psi_1_tiled_var = tf.tile(tf.expand_dims(var_variational, axis=1), [1, N_LATENT_VARIABLES, 1])\n",
    "psi_1_tiled_latent = tf.tile(tf.expand_dims(latent_variables, axis=0), [N_OBS, 1, 1])\n",
    "psi_1_expanded_weights = tf.reshape(ard_weights, shape=[1, 1, N_LATENT_DIMS])\n",
    "\n",
    "# Calculating psi_1 matrix - note that to meet the format I need to multiply over N_LATENT_DIMS\n",
    "psi_1_norm = psi_1_tiled_var * psi_1_expanded_weights + 1\n",
    "psi_1_diff = -0.5 * psi_1_expanded_weights * (psi_1_tiled_mu - psi_1_tiled_latent) ** 2 / psi_1_norm\n",
    "psi_1 = ard_sigma ** 2 * tf.reduce_prod(tf.exp(psi_1_diff) / tf.sqrt(psi_1_norm), axis=2)\n",
    "print psi_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The psi_2 table has [N_LATENT_VARIABLES, N_LATENT_VARIABLES] size\n",
    "# During the construction of psi_2 we need to reduce over N_OBS and N_LATENT_VARIABLES\n",
    "# Expanding all variables to meet [N_LATENT_VARIABLES, N_OBS, N_LATENT_DIMS, N_LATENT_VARIABLES] format\n",
    "psi_2_left_latent = tf.tile(tf.expand_dims(tf.expand_dims(latent_variables, axis=1), axis=-1), [1, N_OBS, 1, N_LATENT_VARIABLES])\n",
    "psi_2_right_latent = tf.tile(tf.expand_dims(tf.expand_dims(tf.transpose(latent_variables), axis=0), axis=0), [N_LATENT_VARIABLES, N_OBS, 1, 1])\n",
    "psi_2_tiled_mu = tf.tile(tf.expand_dims(tf.expand_dims(mu_variational, axis=0), axis=-1), [N_LATENT_VARIABLES, 1, 1, N_LATENT_VARIABLES])\n",
    "psi_2_tiled_var = tf.tile(tf.expand_dims(tf.expand_dims(var_variational, axis=0), axis=-1), [N_LATENT_VARIABLES, 1, 1, N_LATENT_VARIABLES])\n",
    "psi_2_expanded_weights = tf.reshape(ard_weights, shape=[1, 1, N_LATENT_DIMS, 1])\n",
    "\n",
    "# Calculating psi_2 matrix - note that to meet the format I need to multiply over N_LATENT_DIMS and then sum over N_OBS\n",
    "psi_2_norm = 2 * psi_2_tiled_var * psi_2_expanded_weights + 1\n",
    "psi_2_diff = (-0.25 * psi_2_expanded_weights * (psi_2_left_latent - psi_2_right_latent) ** 2 \n",
    "              - psi_2_expanded_weights * (psi_2_tiled_mu - 0.5 * psi_2_left_latent - 0.5 * psi_2_right_latent) ** 2 / psi_2_norm)\n",
    "psi_2 = ard_sigma ** 4 * tf.reduce_sum(tf.reduce_prod(tf.exp(psi_2_diff) / tf.sqrt(psi_2_norm), axis=2), axis=1)\n",
    "print psi_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the variational bound on log-likelihood (with throwing away constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_term = 0.5 * beta * tf.trace(tf.matmul(tf.matrix_inverse(K), psi_2)) - 0.5 * beta * psi_0\n",
    "\n",
    "W = beta - beta ** 2 * tf.matmul(tf.matmul(psi_1, tf.matrix_inverse(beta * psi_2 + K)), tf.transpose(psi_1))\n",
    "log_term = (-0.5 * tf.matmul(tf.matmul(tf.transpose(y_obs), W), y_obs) \n",
    "            + 0.5 * N_OBS * tf.log(beta) \n",
    "            + 0.5 * tf.log(tf.matrix_determinant(K))\n",
    "            - 0.5 * tf.log(tf.matrix_determinant(beta * psi_2 + K))\n",
    "           )\n",
    "\n",
    "# kl_term calculated separately for each component of variational posterior against standard normal distribution\n",
    "# (standard normal is a prior, separate estimation goes after mean-field approach for variational Bayes)\n",
    "# formula used: https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians\n",
    "kl_term = tf.reduce_sum(-tf.log(tf.sqrt(var_variational)) + 0.5 * var_variational + 0.5 * mu_variational ** 2 - 0.5)\n",
    "\n",
    "variational_bound = trace_term + log_term + kl_term\n",
    "grad_step = tf.train.AdamOptimizer(1e-2).minimize(-variational_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "N_GRAD_STEPS = 100\n",
    "for _ in range(N_GRAD_STEPS):\n",
    "    _, cur_bound, cur_beta, cur_sigma, cur_weights = sess.run([grad_step, variational_bound, beta, ard_sigma, ard_weights])\n",
    "    print cur_bound, cur_beta, cur_sigma, cur_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
