{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import GPy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data_generator_2d import TARGET_SIGNALS\n",
    "sys.path.append(\"../../PyDeepGP\")\n",
    "import deepgp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing signal to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TARGET_SIGNALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(0, 1, 100)\n",
    "x2 = np.linspace(0, 1, 100)\n",
    "\n",
    "X1, X2 = np.meshgrid(x1,x2)\n",
    "grid_flat = np.vstack([X1.ravel(), X2.ravel()]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pick = 37\n",
    "f = TARGET_SIGNALS[f_pick].fun\n",
    "print TARGET_SIGNALS[f_pick].x_opt\n",
    "print TARGET_SIGNALS[f_pick].desc\n",
    "\n",
    "z = np.array([f(grid_flat[i, :]) for i in range(grid_flat.shape[0])])\n",
    "plt.pcolor(X1, X2, z.reshape((100, 100)), cmap='RdBu')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching DeepGP and using it for Bayesian optimization (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x = np.random.random((50, 2))\n",
    "\n",
    "sample_y = np.apply_along_axis(TARGET_SIGNALS[1].fun, 1, sample_x)\n",
    "sample_y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern1 = GPy.kern.RBF(5, ARD=True) + GPy.kern.Bias(5)\n",
    "kern2 = GPy.kern.RBF(2, ARD=True) + GPy.kern.Bias(2)\n",
    "\n",
    "model = deepgp.DeepGP(nDims=[1, 5, 2],\n",
    "                      Y=sample_y.reshape(-1, 1),\n",
    "                      X=sample_x,\n",
    "                      kernels=[kern1,kern2], \n",
    "                      num_inducing=10, \n",
    "                      back_constraint=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimize(max_iters=500, messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(grid_flat)\n",
    "pred[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(X1, X2, pred[0].reshape((100, 100)), cmap='RdBu')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_x, ei_std = select_next_point(model, min(sample_y), eps=0.1)\n",
    "print next_x, ei_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_obs(TARGET_SIGNALS[1], next_x, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.obslayer.kern.plot_ARD()\n",
    "model.layer_1.kern.plot_ARD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching DeepGP and using it for Bayesian optimization (function form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_obs(data_generator, x, sigma_obs):\n",
    "    # Sampling a new point from data_generator\n",
    "    return data_generator.sample(x) + sigma_obs * np.random.randn(x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deepgp_model(sample_x, sample_y, n_hidden, hidden_width, num_inducing, max_iters):\n",
    "    # The function to construct a deep Gaussian process model\n",
    "    kernels = []\n",
    "    nDims = [1]\n",
    "    \n",
    "    for _ in range(n_hidden):\n",
    "        kernels.append(GPy.kern.RBF(hidden_width, ARD=True) + GPy.kern.Bias(hidden_width))\n",
    "        nDims.append(hidden_width)\n",
    "        \n",
    "    # Using the fact we are dealing with 2D domains\n",
    "    kernels.append(GPy.kern.RBF(2, ARD=True) + GPy.kern.Bias(2))\n",
    "    nDims.append(2)\n",
    "    \n",
    "    # Constructing and training a model\n",
    "    model = deepgp.DeepGP(nDims=nDims,\n",
    "                          Y=sample_y.reshape(-1, 1),\n",
    "                          X=sample_x,\n",
    "                          kernels=kernels, \n",
    "                          num_inducing=num_inducing, \n",
    "                          back_constraint=False\n",
    "                         )    \n",
    "    \n",
    "    model.optimize(max_iters=max_iters, messages=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_next_point(model, best_score, eps=0.1):\n",
    "    # The function to select which point to explore in Bayesian optimization\n",
    "    # The criterion I select is expected improvement\n",
    "    # The way to find a best point to explore is iterative naive search:\n",
    "    # I start with a exhaustive search over a coarse grid over [0, 1] x [0, 1] interval\n",
    "    # Then I do a secondary exhaustive search over a smaller-eps grid centered on optimum from first run\n",
    "\n",
    "    x1 = np.linspace(0, 1, 100)\n",
    "    x2 = np.linspace(0, 1, 100)\n",
    "\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    grid_flat = np.vstack([X1.ravel(), X2.ravel()]).transpose()\n",
    "    \n",
    "    # First iteration of exhaustive search, shuffling to avoid initial stickiness to a constant with noise\n",
    "    np.random.shuffle(grid_flat)\n",
    "    mu, std_1d = model.predict(grid_flat)\n",
    "\n",
    "    z = (best_score - mu) / std_1d\n",
    "    ei = std_1d * scipy.stats.norm.pdf(z) + z * std_1d * scipy.stats.norm.cdf(z)\n",
    "    \n",
    "    # Recording a sanity metric: how variable ei is assumed to be\n",
    "    ei_std = ei.std()\n",
    "    \n",
    "    # Fetching the most promising point and iterating further\n",
    "    x1_center_refined, x2_center_refined = grid_flat[np.argmax(ei, axis=0), :][0]\n",
    "\n",
    "    x1 = np.linspace(max(x1_center_refined - eps, 0), min(x1_center_refined + eps, 1), 100)\n",
    "    x2 = np.linspace(max(x2_center_refined - eps, 0), min(x2_center_refined + eps, 1), 100)\n",
    "\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    grid_flat = np.vstack([X1.ravel(), X2.ravel()]).transpose()\n",
    "    \n",
    "    np.random.shuffle(grid_flat)\n",
    "    mu, std_1d = model.predict(grid_flat)\n",
    "\n",
    "    z = (best_score - mu) / std_1d\n",
    "    ei = std_1d * scipy.stats.norm.pdf(z) + z * std_1d * scipy.stats.norm.cdf(z)\n",
    "    \n",
    "    return grid_flat[np.argmax(ei, axis=0), :][0].reshape(1, -1), ei_std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_bayes_opt_run(data_generator, n_samples_total, n_samples_init, \n",
    "                       sigma_obs, n_hidden, hidden_width, num_inducing, max_iters):\n",
    "    # Starting with a random sample to iterate firther\n",
    "    # (Fixing seed to have a more relevant benchmark)\n",
    "    np.random.seed(123456)\n",
    "    sample_x = np.random.random((n_samples_init, 2))\n",
    "    sample_y = sample_obs(data_generator, sample_x, sigma_obs).reshape(-1, 1)\n",
    "    best_score = np.min(sample_y)\n",
    "    \n",
    "    ei_std_list = []\n",
    "    \n",
    "    for _ in range(n_samples_total - n_samples_init):\n",
    "        deepgp_model = train_deepgp_model(sample_x, sample_y, n_hidden, hidden_width, num_inducing, max_iters)\n",
    "        next_x, ei_std = select_next_point(deepgp_model, best_score)\n",
    "        # print \"Current best: \", \n",
    "        # print \"Next x: \", next_x\n",
    "        ei_std_list.append(ei_std)\n",
    "        \n",
    "        next_y = sample_obs(data_generator, next_x, sigma_obs)\n",
    "        \n",
    "        sample_x = np.vstack([sample_x, next_x])\n",
    "        sample_y = np.vstack([sample_y, next_y])\n",
    "        \n",
    "        best_score = min(best_score, next_y)\n",
    "\n",
    "    return sample_y, ei_std_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, ei_std = deep_bayes_opt_run(TARGET_SIGNALS[3], 150, 25, 0.01, 3, 3, 25, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
