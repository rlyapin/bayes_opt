{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import GPy\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data_generator_2d import TARGET_SIGNALS\n",
    "sys.path.append(\"../../PyDeepGP\")\n",
    "import deepgp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing signal to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TARGET_SIGNALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(0, 1, 100)\n",
    "x2 = np.linspace(0, 1, 100)\n",
    "\n",
    "X1, X2 = np.meshgrid(x1,x2)\n",
    "grid_flat = np.vstack([X1.ravel(), X2.ravel()]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pick = 3\n",
    "f = TARGET_SIGNALS[f_pick].fun\n",
    "print TARGET_SIGNALS[f_pick].x_opt\n",
    "print TARGET_SIGNALS[f_pick].desc\n",
    "\n",
    "z = np.array([f(grid_flat[i, :]) for i in range(grid_flat.shape[0])])\n",
    "plt.pcolor(X1, X2, z.reshape((100, 100)), cmap='RdBu')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picks = range(0, 36, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picks = range(0, 36, 3)\n",
    "counter = 0\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(16, 9))\n",
    "for ax in axes.flat:\n",
    "    f_pick = picks[counter]\n",
    "    f = TARGET_SIGNALS[f_pick].fun\n",
    "    \n",
    "    z = np.array([f(grid_flat[i, :]) for i in range(grid_flat.shape[0])])\n",
    "    \n",
    "    ax.set_axis_off()\n",
    "    im = ax.pcolor(X1, X2, z.reshape((100, 100)), cmap='RdBu')\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "fig.colorbar(im, ax=axes.ravel().tolist())\n",
    "\n",
    "plt.suptitle(\"Examples of test functions\", fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching DeepGP and using it for Bayesian optimization (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x = np.random.random((50, 2))\n",
    "\n",
    "sample_y = np.apply_along_axis(TARGET_SIGNALS[f_pick].fun, 1, sample_x)\n",
    "sample_y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern1 = GPy.kern.RBF(5, ARD=True) + GPy.kern.Bias(5)\n",
    "kern2 = GPy.kern.RBF(2, ARD=True) + GPy.kern.Bias(2)\n",
    "\n",
    "model = deepgp.DeepGP(nDims=[1, 5, 2],\n",
    "                      Y=sample_y.reshape(-1, 1),\n",
    "                      X=sample_x,\n",
    "                      kernels=[kern1,kern2], \n",
    "                      num_inducing=25, \n",
    "                      back_constraint=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimize(max_iters=500, messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(grid_flat)\n",
    "pred[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(X1, X2, pred[0].reshape((100, 100)), cmap='RdBu')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_x, ei_std = select_next_point(model, min(sample_y), eps=0.1)\n",
    "print next_x, ei_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_obs(TARGET_SIGNALS[1], next_x, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.obslayer.kern.plot_ARD()\n",
    "model.layer_1.kern.plot_ARD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = GPy.kern.RBF(input_dim=2, ARD=True)\n",
    "model = GPy.models.GPRegression(sample_x, sample_y.reshape(-1, 1), kernel)\n",
    "model.optimize()\n",
    "\n",
    "model.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching DeepGP and using it for Bayesian optimization (function form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_obs(data_generator, x, sigma_obs):\n",
    "    # Sampling a new point from data_generator\n",
    "    return data_generator.sample(x) + sigma_obs * np.random.randn(x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deepgp_model(sample_x, sample_y, n_hidden, hidden_width, num_inducing, max_iters):\n",
    "    # The function to construct a deep Gaussian process model\n",
    "    kernels = []\n",
    "    nDims = [1]\n",
    "    \n",
    "    for _ in range(n_hidden):\n",
    "        kernels.append(GPy.kern.RBF(hidden_width, ARD=True) + GPy.kern.Bias(hidden_width))\n",
    "        nDims.append(hidden_width)\n",
    "        \n",
    "    # Using the fact we are dealing with 2D domains\n",
    "    kernels.append(GPy.kern.RBF(2, ARD=True) + GPy.kern.Bias(2))\n",
    "    nDims.append(2)\n",
    "    \n",
    "    # Constructing and training a model\n",
    "    model = deepgp.DeepGP(nDims=nDims,\n",
    "                          Y=sample_y.reshape(-1, 1),\n",
    "                          X=sample_x,\n",
    "                          kernels=kernels, \n",
    "                          num_inducing=num_inducing, \n",
    "                          back_constraint=False\n",
    "                         )    \n",
    "    \n",
    "    model.optimize(max_iters=max_iters, messages=False)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sparsegp_model(sample_x, sample_y, num_inducing, max_iters):\n",
    "    # The function to construct and train sparse Gaussian process model  \n",
    "    kern = GPy.kern.RBF(2, ARD=True) + GPy.kern.Bias(2)\n",
    "    \n",
    "    model = GPy.models.SparseGPRegression(X=sample_x, Y=sample_y, kernel=kern, num_inducing=num_inducing)\n",
    "    \n",
    "    model.optimize(max_iters=max_iters, messages=False)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_next_point(model, best_score, eps=0.1):\n",
    "    # The function to select which point to explore in Bayesian optimization\n",
    "    # The criterion I select is expected improvement\n",
    "    # The way to find a best point to explore is iterative naive search:\n",
    "    # I start with a exhaustive search over a coarse grid over [0, 1] x [0, 1] interval\n",
    "    # Then I do a secondary exhaustive search over a smaller-eps grid centered on optimum from first run\n",
    "\n",
    "    x1 = np.linspace(0, 1, 100)\n",
    "    x2 = np.linspace(0, 1, 100)\n",
    "\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    grid_flat = np.vstack([X1.ravel(), X2.ravel()]).transpose()\n",
    "    \n",
    "    # First iteration of exhaustive search, shuffling to avoid initial stickiness to a constant with noise\n",
    "    np.random.shuffle(grid_flat)\n",
    "    mu, std_1d = model.predict(grid_flat)\n",
    "\n",
    "    z = (best_score - mu) / std_1d\n",
    "    ei = std_1d * scipy.stats.norm.pdf(z) + z * std_1d * scipy.stats.norm.cdf(z)\n",
    "    \n",
    "    # Recording a sanity metric: how variable ei is assumed to be\n",
    "    ei_std = ei.std()\n",
    "    \n",
    "    # Fetching the most promising point and iterating further\n",
    "    x1_center_refined, x2_center_refined = grid_flat[np.argmax(ei, axis=0), :][0]\n",
    "\n",
    "    x1 = np.linspace(max(x1_center_refined - eps, 0), min(x1_center_refined + eps, 1), 100)\n",
    "    x2 = np.linspace(max(x2_center_refined - eps, 0), min(x2_center_refined + eps, 1), 100)\n",
    "\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    grid_flat = np.vstack([X1.ravel(), X2.ravel()]).transpose()\n",
    "    \n",
    "    np.random.shuffle(grid_flat)\n",
    "    mu, std_1d = model.predict(grid_flat)\n",
    "\n",
    "    z = (best_score - mu) / std_1d\n",
    "    ei = std_1d * scipy.stats.norm.pdf(z) + z * std_1d * scipy.stats.norm.cdf(z)\n",
    "    \n",
    "    return grid_flat[np.argmax(ei, axis=0), :][0].reshape(1, -1), ei_std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_x_opt(model, eps=0.1):\n",
    "    # The function to select the potential optimum in the end of Bayesian optimization\n",
    "    \n",
    "    x1 = np.linspace(0, 1, 100)\n",
    "    x2 = np.linspace(0, 1, 100)\n",
    "\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    grid_flat = np.vstack([X1.ravel(), X2.ravel()]).transpose()\n",
    "\n",
    "    np.random.shuffle(grid_flat)\n",
    "    mu, _ = model.predict(grid_flat)\n",
    "\n",
    "    # Fetching the most promising point and iterating further\n",
    "    x1_center_refined, x2_center_refined = grid_flat[np.argmin(mu, axis=0), :][0]\n",
    "\n",
    "    x1 = np.linspace(max(x1_center_refined - eps, 0), min(x1_center_refined + eps, 1), 100)\n",
    "    x2 = np.linspace(max(x2_center_refined - eps, 0), min(x2_center_refined + eps, 1), 100)\n",
    "\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    grid_flat = np.vstack([X1.ravel(), X2.ravel()]).transpose()\n",
    "\n",
    "    np.random.shuffle(grid_flat)\n",
    "    mu, _ = model.predict(grid_flat)\n",
    "        \n",
    "    return grid_flat[np.argmin(mu, axis=0), :][0].reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_opt_run(data_generator, n_samples_total, n_samples_init, is_deep,\n",
    "                       sigma_obs, n_hidden, hidden_width, num_inducing, max_iters):\n",
    "    # Starting with a random sample to iterate firther\n",
    "    # (Fixing seed to have a more relevant benchmark)\n",
    "    np.random.seed(123456)\n",
    "    sample_x = np.random.random((n_samples_init, 2))\n",
    "    sample_y = sample_obs(data_generator, sample_x, sigma_obs).reshape(-1, 1)\n",
    "    best_score = np.min(sample_y)\n",
    "    \n",
    "    ei_std_list = []\n",
    "    \n",
    "    for _ in range(n_samples_total - n_samples_init):\n",
    "        if is_deep:\n",
    "            model = train_deepgp_model(sample_x, sample_y, n_hidden, hidden_width, num_inducing, max_iters)\n",
    "        else:\n",
    "            model = train_sparsegp_model(sample_x, sample_y, num_inducing, max_iters)\n",
    "            \n",
    "        next_x, ei_std = select_next_point(model, best_score)\n",
    "        ei_std_list.append(ei_std)\n",
    "        \n",
    "        next_y = sample_obs(data_generator, next_x, sigma_obs)\n",
    "        \n",
    "        sample_x = np.vstack([sample_x, next_x])\n",
    "        sample_y = np.vstack([sample_y, next_y])\n",
    "        \n",
    "        best_score = min(best_score, next_y)\n",
    "        \n",
    "    best_x = select_x_opt(model)\n",
    "\n",
    "    return best_x, sample_y, ei_std_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, ei_std = bayes_opt_run(TARGET_SIGNALS[3], 35, 25, 0, 0.01, 3, 3, 25, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running simulations for all signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"deepGP_1_5_opt_results.txt\", \"a\") as f:\n",
    "    \n",
    "    for signal in TARGET_SIGNALS:\n",
    "                \n",
    "        print \"Optimizing \" + signal.desc\n",
    "\n",
    "        n_samples_init = 25\n",
    "        n_samples_total = 200\n",
    "        n_hidden = 1\n",
    "        hidden_width = 5\n",
    "        sigma_obs = 1e-8\n",
    "\n",
    "        best_x, y, ei_std = bayes_opt_run(signal, n_samples_total, n_samples_init, 1, sigma_obs, \n",
    "                                          n_hidden, hidden_width, 25, 100)\n",
    "\n",
    "        print \"Best x: \"\n",
    "        print best_x\n",
    "\n",
    "#         x_opt_distance = min(np.linalg.norm(x[n_samples_init:, :] - signal.x_opt, ord=2, axis=1))\n",
    "        x_opt_distance = np.sqrt((best_x[0] - signal.x_opt[0]) ** 2 + (best_x[1] - signal.x_opt[1]) ** 2)\n",
    "\n",
    "        sim_results = {}\n",
    "        sim_results[\"model\"] = signal.desc\n",
    "        sim_results[\"n_samples_init\"] = n_samples_init\n",
    "        sim_results[\"n_samples_total\"] = n_samples_total\n",
    "        sim_results[\"sigma_obs\"] = sigma_obs\n",
    "        sim_results[\"n_hidden\"] = n_hidden\n",
    "        sim_results[\"hidden_width\"] = hidden_width\n",
    "        sim_results[\"x_opt_distance\"] = x_opt_distance\n",
    "        sim_results[\"scores\"] = list(y.reshape(-1,))\n",
    "        sim_report = json.dumps(sim_results)\n",
    "\n",
    "        f.write(sim_report + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing deep and sparse models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_1_5_rewards = np.zeros((40, 200))\n",
    "counter = 0\n",
    "\n",
    "with open(\"deepGP_1_5_opt_results.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        sim_results = json.loads(line)\n",
    "        deep_1_5_rewards[counter, :] = sim_results[\"scores\"]\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_1_2_rewards = np.zeros((39, 200))\n",
    "counter = 0\n",
    "\n",
    "with open(\"deepGP_1_2_opt_results.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        sim_results = json.loads(line)\n",
    "        deep_1_2_rewards[counter, :] = sim_results[\"scores\"]\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_5_2_rewards = np.zeros((40, 200))\n",
    "counter = 0\n",
    "\n",
    "with open(\"deepGP_opt_results.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        sim_results = json.loads(line)\n",
    "        deep_5_2_rewards[counter, :] = sim_results[\"scores\"]\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_rewards = np.zeros((40, 200))\n",
    "counter = 0\n",
    "\n",
    "with open(\"sparseGP_opt_results.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        sim_results = json.loads(line)\n",
    "        sparse_rewards[counter, :] = sim_results[\"scores\"]\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "plt.title(\"Average regrets during Bayesian optimization\")\n",
    "plt.plot(moving_average(sparse_rewards.mean(axis=0), 10), label=\"baseline (ma)\")\n",
    "plt.plot(moving_average(deep_1_2_rewards.mean(axis=0), 10), label=\"deep_1h_2w (ma)\")\n",
    "plt.plot(moving_average(deep_1_5_rewards.mean(axis=0), 10), label=\"deep_1h_5w (ma)\")\n",
    "plt.plot(moving_average(deep_5_2_rewards.mean(axis=0), 10), label=\"deep_5h_2w (ma)\")\n",
    "plt.xlabel(\"timestamp\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(deep_rewards.mean(axis=1), sparse_rewards.mean(axis=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(0, 1, 100)\n",
    "x2 = np.linspace(0, 1, 100)\n",
    "\n",
    "X1, X2 = np.meshgrid(x1,x2)\n",
    "grid_flat = np.vstack([X1.ravel(), X2.ravel()]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pick = 3\n",
    "f = TARGET_SIGNALS[f_pick].fun\n",
    "print TARGET_SIGNALS[f_pick].x_opt\n",
    "print TARGET_SIGNALS[f_pick].desc\n",
    "\n",
    "z = np.array([f(grid_flat[i, :]) for i in range(grid_flat.shape[0])])\n",
    "plt.pcolor(X1, X2, z.reshape((100, 100)), cmap='RdBu')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 5000\n",
    "NUM_INDUCING = 50\n",
    "MAX_ITERS = 1000\n",
    "f_pick = 3\n",
    "\n",
    "np.random.seed(123456)\n",
    "sample_x = np.random.random((N_SAMPLES, 2))\n",
    "sample_y = np.apply_along_axis(TARGET_SIGNALS[f_pick].fun, 1, sample_x)\n",
    "\n",
    "\n",
    "z = np.array([TARGET_SIGNALS[f_pick].fun(grid_flat[i, :]) for i in range(grid_flat.shape[0])])\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(X1, X2, z.reshape((100, 100)), cmap='RdBu')\n",
    "\n",
    "\n",
    "model = train_sparsegp_model(sample_x, sample_y.reshape(-1, 1), \n",
    "                             num_inducing=NUM_INDUCING, max_iters=MAX_ITERS)\n",
    "pred = model.predict(grid_flat)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(X1, X2, pred[0].reshape((100, 100)), cmap='RdBu')\n",
    "\n",
    "model = train_deepgp_model(sample_x, sample_y.reshape(-1, 1), n_hidden=1, hidden_width=2, \n",
    "                           num_inducing=NUM_INDUCING, max_iters=MAX_ITERS)\n",
    "pred = model.predict(grid_flat)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(X1, X2, pred[0].reshape((100, 100)), cmap='RdBu')\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
