{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook is dedicated to sparse GP and their training / inference (as a prerequisite to the paper on deep GP)\n",
    "\n",
    "Specifically, I am following these papers:\n",
    "\n",
    "Titsias - Variational Learning of Inducing Variables in Sparse Gaussian\n",
    "Processes: http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf\n",
    "\n",
    "Titsias - Variational Model Selection for Sparse Gaussian\n",
    "Process Regression (more details compared to the previous one): https://pdfs.semanticscholar.org/db7b/e492a629a98db7f9d77d552fd3568ff42189.pdf\n",
    "\n",
    "Note that for now I am mostly interested in inference so I am not going for optiml selection of latent points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse GP setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below samples a random function, picks observations from this random function to train a sparse GP and trains a sparse GP for a smaller subset of potential observations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 1000)\n",
    "true_y = np.random.randn(1) * np.ones(1000)\n",
    "for i in range(1, 6):\n",
    "    true_y += np.random.randn(1) * np.sin(i * math.pi * x)\n",
    "    true_y += np.random.randn(1) * np.cos(i * math.pi * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGMA_OBS = 0.5\n",
    "N_SAMPLES = 100\n",
    "N_SPARSE = 10\n",
    "KERNEL_SCALE = 0.1\n",
    "\n",
    "# Getting a \"true\" signal\n",
    "pick_x = np.random.choice(range(len(x)), size=N_SAMPLES, replace=False)\n",
    "sample_x = x[pick_x]\n",
    "sample_y = true_y[pick_x] + SIGMA_OBS * np.random.randn(N_SAMPLES)\n",
    "\n",
    "# Picking random locations for \"sparse\" latent signal (note that true values are missing)\n",
    "pick_x = np.random.choice(range(len(x)), size=N_SPARSE, replace=False)\n",
    "sparse_x = x[pick_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling latent signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_gaussian(x1, x2):\n",
    "    # Note that compared to previous notebook I hardcoded kernel width\n",
    "    \n",
    "    x1_matrix = np.tile(x1, len(x2)).reshape((len(x2), len(x1)))\n",
    "    x2_matrix = np.tile(x2, len(x1)).reshape((len(x1), len(x2))).transpose()\n",
    "    \n",
    "    k_matrix = np.exp(-(x1_matrix - x2_matrix) ** 2 / (2 * KERNEL_SCALE * KERNEL_SCALE))\n",
    "    \n",
    "    return k_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_matrix = np.linalg.inv(k_gaussian(sparse_x, sparse_x) \n",
    "                             + np.dot(k_gaussian(sample_x, sparse_x), k_gaussian(sparse_x, sample_x)) / SIGMA_OBS ** 2)\n",
    "sparse_mu = reduce(np.dot, [k_gaussian(sparse_x, sparse_x), sigma_matrix, k_gaussian(sample_x, sparse_x), sample_y]) / SIGMA_OBS ** 2\n",
    "sparse_cov = reduce(np.dot, [k_gaussian(sparse_x, sparse_x), sigma_matrix, k_gaussian(sparse_x, sparse_x)])\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        sparse_chol = np.linalg.cholesky(sparse_cov)\n",
    "        break\n",
    "    except:\n",
    "        sparse_cov += 0.0001 * np.eye(len(sparse_x))\n",
    "        \n",
    "sparse_y = sparse_mu + np.dot(sparse_chol, np.random.randn(len(sparse_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance of the conditional prior (~noise added due to sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_sparse = np.dot(k_gaussian(sparse_x, sample_x),\n",
    "                  np.linalg.inv(k_gaussian(sparse_x, sparse_x) + np.eye(len(sparse_x)) * SIGMA_OBS ** 2)\n",
    "                 )\n",
    "\n",
    "posterior_sigma = (k_gaussian(sample_x, sample_x) - \n",
    "                   np.dot(K_sparse, k_gaussian(sample_x, sparse_x)))\n",
    "\n",
    "print np.trace(posterior_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_posterior(sample_x, sample_y, x, sigma_obs):\n",
    "    # Calculating posterior for gaussian processes\n",
    "    # I am specifically interested in posterior mean, std and cholesky matrix for postrior at sampled points (for nei)\n",
    "    # it is assumed that observations have some additional gaussian noise\n",
    "    \n",
    "    # Important: the method cannot handle sigma_obs=0 if I want to predict for sample_x\n",
    "    # Mostly numerical issues: with zero noise matrix to invert may not be positive-semidefinite\n",
    "    \n",
    "    # Separately calculating matrix used to calculate both mean and variance\n",
    "    K = np.dot(k_gaussian(sample_x, x),\n",
    "               np.linalg.inv(k_gaussian(sample_x, sample_x) + np.eye(len(sample_x)) * sigma_obs ** 2)\n",
    "              )\n",
    "    \n",
    "    mu = np.dot(K, sample_y)\n",
    "    sigma = k_gaussian(x, x) - np.dot(K, k_gaussian(x, sample_x))\n",
    "    std_1d = np.sqrt([sigma[i, i] for i in range(len(mu))])\n",
    "    \n",
    "    return mu.reshape(-1), std_1d.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_posterior_mu, true_posterior_std = gp_posterior(sample_x, sample_y, x, SIGMA_OBS)\n",
    "sparse_posterior_mu, sparse_posterior_std = gp_posterior(sparse_x, sparse_y, x, SIGMA_OBS)\n",
    "\n",
    "plt.plot(x, true_y, color=\"b\", label=\"true signal\")\n",
    "plt.plot(sample_x, sample_y, \".\", color=\"b\", label=\"sampled_signal\")\n",
    "plt.plot(sparse_x, sparse_y, \".\", color=\"r\", label=\"latent_signal\")\n",
    "plt.fill_between(x, true_posterior_mu - 2 * true_posterior_std, \n",
    "                 true_posterior_mu + 2 * true_posterior_std, color=\"b\", alpha=0.25, label=\"true_posterior\")\n",
    "plt.fill_between(x, sparse_posterior_mu - 2 * sparse_posterior_std, \n",
    "                 sparse_posterior_mu + 2 * sparse_posterior_std, color=\"r\", alpha=0.25, label=\"sparse_posterior\")\n",
    "plt.title(\"Sparse GP inference\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
