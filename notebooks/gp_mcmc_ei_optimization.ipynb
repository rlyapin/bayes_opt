{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tries MCMC sampling of kernel hyperparameters of GP and goes for average EI in Bayesian optimization (as suggested in https://arxiv.org/pdf/1206.2944.pdf)\n",
    "\n",
    "For simplicity considering 1D signal and 1 kernel hyperparameter (length for Gaussian kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy.stats\n",
    "import time\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 1000)\n",
    "true_y = np.random.randn(1) * np.ones(1000)\n",
    "for i in range(1, 6):\n",
    "    true_y += np.random.randn(1) * np.sin(i * math.pi * x)\n",
    "    true_y += np.random.randn(1) * np.cos(i * math.pi * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard machinery for GP inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_gaussian(x1, x2, l=0.1):\n",
    "    # calculating gaussian kernel matrix\n",
    "    # the output has shape (len(x2), len(x1))\n",
    "    # entry at [i, j] position is given by k(x2[i], x1[j])\n",
    "    # dealing with gaussian kernels so k(x, y) = e ^ ((x - y) ** 2 / 2 * l ** 2)\n",
    "    \n",
    "    # gaussian kernel hyperparameters - adjusts the distance between points and variance\n",
    "    scale_kernel = 1\n",
    "    \n",
    "    x1_matrix = np.tile(x1, len(x2)).reshape((len(x2), len(x1)))\n",
    "    x2_matrix = np.tile(x2, len(x1)).reshape((len(x1), len(x2))).transpose()\n",
    "    \n",
    "    k_matrix = np.exp(-(x1_matrix - x2_matrix) ** 2 / (2 * l * l)) * scale_kernel ** 2\n",
    "    \n",
    "    return k_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(sample_x, sample_y, sigma_obs, l):\n",
    "    # The following function calculates the log-likelihood of observed data\n",
    "    # wrt to prior distribution for GP, i.e. zero mean and sigma given by k_gaussian(l) + sigma_obs ** 2 * I\n",
    "    \n",
    "    # Under that model the log-likelihood is given by \n",
    "    # -0.5 * y' * sigma(-1) * y - 0.5 * n * log(2pi) - 0.5 * log|sigma|\n",
    "    \n",
    "    # To make sense of the code below note that we express log-likelihood through the cholesky decomposition of sigma\n",
    "    # Then |sigma| = |chol| ** 2 (det of product is product of det)\n",
    "    # |chol| = prod(chol_ii) (because cholesky matrix is lower triangular)\n",
    "    # Thus, 0.5 * log|sigma| = sum(log(chol_ii))\n",
    "    \n",
    "    sigma = k_gaussian(sample_x, sample_x, l) + np.eye(len(sample_x)) * sigma_obs ** 2\n",
    "    chol = np.linalg.cholesky(sigma)\n",
    "    \n",
    "    # Calculating alpha = sigma(-1) * y (or solution to sigma * alpha = y) using cholesky matrix\n",
    "    # (This trick is taken from sklearn implementation of GP)\n",
    "    alpha = sp.linalg.cho_solve((chol, True), sample_y).reshape((-1, 1))\n",
    "    \n",
    "    log_lik = -0.5 * np.dot(sample_y.reshape(1, -1), alpha)\n",
    "    log_lik -= 0.5 * len(sample_x) * np.log(2 * np.pi)\n",
    "    log_lik -= np.trace(np.log(np.absolute(chol)))\n",
    "    \n",
    "    return log_lik[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_posterior(sample_x, sample_y, x, sigma_obs, l):\n",
    "    # Calculating posterior for gaussian processes\n",
    "    # I am specifically interested in posterior mean, std and cholesky matrix for postrior at sampled points (for nei)\n",
    "    # it is assumed that observations have some additional gaussian noise\n",
    "    \n",
    "    # Important: the method cannot handle sigma_obs=0 if I want to predict for sample_x\n",
    "    # Mostly numerical issues: with zero noise matrix to invert may not be positive-semidefinite\n",
    "    \n",
    "    # Separately calculating matrix used to calculate both mean and variance\n",
    "    K = np.dot(k_gaussian(sample_x, x, l),\n",
    "               np.linalg.inv(k_gaussian(sample_x, sample_x, l) + np.eye(len(sample_x)) * sigma_obs ** 2)\n",
    "              )\n",
    "    \n",
    "    mu = np.dot(K, sample_y)\n",
    "    sigma = k_gaussian(x, x, l) - np.dot(K, k_gaussian(x, sample_x, l))\n",
    "    std_1d = np.sqrt([sigma[i, i] for i in range(len(mu))])\n",
    "    \n",
    "    return mu.reshape(-1), std_1d.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking a uniform [0, 1] prior for the kernel hyperparameter (10 ** (-8) is added for more stability during MCMC)\n",
    "l_prior = lambda l: int(l > 0 and l < 1) + 10 ** (-8) * int(l <= 0 or l >= 1)\n",
    "# For simplicity specifying inverse cdf as well\n",
    "l_icdf = lambda l: l\n",
    "# Specifying jumping distribution - has to be symmetric to be viable for several MCMC algos (e.g. Metropolis-Hastings)\n",
    "l_jump = lambda l: l + 0.05 * np.random.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCMC_l_sampler(sample_x, sample_y, sigma_obs, \n",
    "                   l_prior, l_icdf, l_jump,\n",
    "                   burn_period, mcmc_samples):\n",
    "    \n",
    "    # This function performs Bayesian MCMC sampling for Gaussian kernel hyperparameters\n",
    "    # Specifically, the first point is sampled using inverse cdf for l_prior\n",
    "    # Moves are suggested using l_jump function\n",
    "    # Moves are accepted / rejected with Metropolis-Hastings algorithms\n",
    "    # (i.e. true posterior density is proportional to exp(log_likelihood) * l_prior, \n",
    "    # ratio of posterior values give the probability of acception a move)\n",
    "    # First burn_period samples of l are discarded and n_samples consecutive samples are the output of a function\n",
    "    \n",
    "    # MCMC is concerned with the ratio of true probabilities\n",
    "    # However, for efficiency reasons we express everything through log-lokelihoods\n",
    "    log_posterior = lambda l: log_likelihood(sample_x, sample_y, sigma_obs, l) + np.log(l_prior(l))\n",
    "    \n",
    "    l = l_icdf(np.random.rand())\n",
    "    past_log_posterior = log_posterior(l)\n",
    "    for _ in range(burn_period):\n",
    "        next_l = l_jump(l)\n",
    "        next_log_posterior = log_posterior(next_l)\n",
    "        if np.log(np.random.randn()) < (next_log_posterior - past_log_posterior):\n",
    "            l = next_l\n",
    "            past_log_posterior = next_log_posterior\n",
    "            \n",
    "    sampled_l = []\n",
    "    for _ in range(mcmc_samples):\n",
    "        next_l = l_jump(l)\n",
    "        next_log_posterior = log_posterior(next_l)\n",
    "        if np.log(np.random.randn()) < (next_log_posterior - past_log_posterior):\n",
    "            l = next_l\n",
    "            past_log_posterior = next_log_posterior\n",
    "        sampled_l.append(l)\n",
    "\n",
    "    return sampled_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_obs = 0.5\n",
    "\n",
    "pick_x = np.random.choice(range(len(x)), size=5, replace=False)\n",
    "sample_x = x[pick_x]\n",
    "sample_y = true_y[pick_x] + sigma_obs * np.random.randn(5)\n",
    "\n",
    "sampled_l = MCMC_l_sampler(sample_x, sample_y, sigma_obs, \n",
    "                           l_prior, l_icdf, l_jump,\n",
    "                           burn_period=10000, mcmc_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(np.linspace(0.01,1,100), \n",
    "         [np.exp(log_likelihood(sample_x, sample_y, sigma_obs, l)) for l in np.linspace(0.01,1,100)])\n",
    "plt.title(\"True density of posterior distribution (not scaled)\")\n",
    "plt.xlabel(\"l\")\n",
    "plt.ylabel(\"likelihood\")\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.hist(sampled_l, bins=25, normed=1)\n",
    "plt.title(\"Density of MCMC sampled kernel hyperparameters\")\n",
    "plt.xlim([0, 1])\n",
    "plt.xlabel(\"l\")\n",
    "plt.ylabel(\"Normalized frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EI optimization section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_obs = 0.5\n",
    "burn_period = 10000\n",
    "mcmc_samples = 25\n",
    "l_prior = lambda l: int(l > 0 and l < 1) + 10 ** (-8) * int(l <= 0 or l >= 1)\n",
    "l_icdf = lambda l: l\n",
    "l_jump = lambda l: l + 0.05 * np.random.randn()\n",
    "\n",
    "\n",
    "pick_x = np.random.choice(range(len(x)), size=3, replace=False)\n",
    "sample_x = x[pick_x]\n",
    "sample_y = true_y[pick_x] + sigma_obs * np.random.randn(3)\n",
    "best_y = np.max(sample_y)\n",
    "\n",
    "for t in range(1, 100):\n",
    "    \n",
    "    # Sampling possible kernel hyprparameters\n",
    "    sampled_l = MCMC_l_sampler(sample_x, sample_y, sigma_obs, \n",
    "                               l_prior, l_icdf, l_jump,\n",
    "                               burn_period, mcmc_samples)\n",
    "    \n",
    "    # Averaging GP posterior and EI over possible kernel hyperparameters\n",
    "    # Note that as std is not quite an expectation, its averaging is a hack and not necessariy would give true std\n",
    "    mu = np.zeros((len(x),))\n",
    "    std_1d = np.zeros((len(x),))\n",
    "    ei = np.zeros((len(x),))\n",
    "    for l in sampled_l:\n",
    "        sampled_mu, sampled_std_1d = gp_posterior(sample_x, sample_y, x, sigma_obs, l)\n",
    "\n",
    "        z = (sampled_mu - best_y) / sampled_std_1d\n",
    "        sampled_ei = sampled_std_1d * scipy.stats.norm.pdf(z) + z * sampled_std_1d * scipy.stats.norm.cdf(z)\n",
    "        \n",
    "        mu += sampled_mu\n",
    "        std_1d += sampled_std_1d\n",
    "        ei += sampled_ei\n",
    "    \n",
    "\n",
    "    pick_x = np.argmax(ei)\n",
    "    sample_x = np.append(sample_x, x[pick_x])\n",
    "    sample_y = np.append(sample_y, true_y[pick_x] + sigma_obs * np.random.randn(1))\n",
    "    best_y = max(sample_y[-1], best_y)\n",
    "    \n",
    "    mu /= mcmc_samples\n",
    "    std_1d /= mcmc_samples\n",
    "        \n",
    "    plt.plot(x, true_y, label=\"true_signal\")\n",
    "    plt.plot(sample_x, sample_y, \".\", color=\"r\", label=\"picked_x\")\n",
    "    plt.plot([sample_x[-1]], [sample_y[-1]], \".\", color=\"b\", label=\"last_x\")\n",
    "    plt.plot(x, mu, color=\"g\", label=\"posterior\")\n",
    "    plt.fill_between(x, mu - 2 * std_1d, mu + 2 * std_1d, color=\"g\", alpha=0.5)\n",
    "\n",
    "    plt.title(\"True and recovered signals\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.savefig(\"images/mcmc_iter_{}.png\".format(t))\n",
    "    plt.show()\n",
    "\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "images = []\n",
    "filenames = [\"images/mcmc_iter_{}.png\".format(t) for t in range(1, 80)]\n",
    "for filename in filenames:\n",
    "    images.append(imageio.imread(filename))\n",
    "imageio.mimsave('images/mcmc_opt.gif', images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
