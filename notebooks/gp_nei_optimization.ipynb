{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of the following paper: https://arxiv.org/pdf/1706.07094.pdf\n",
    "\n",
    "NEI stands for noisy expected improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EI without noise correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy.stats\n",
    "import time\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 1000)\n",
    "true_y = np.random.randn(1) * np.ones(1000)\n",
    "for i in range(1, 6):\n",
    "    true_y += np.random.randn(1) * np.sin(i * math.pi * x)\n",
    "    true_y += np.random.randn(1) * np.cos(i * math.pi * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_gaussian(x1, x2, l=0.1):\n",
    "    # calculating gaussian kernel matrix\n",
    "    # the output has shape (len(x2), len(x1))\n",
    "    # entry at [i, j] position is given by k(x2[i], x1[j])\n",
    "    # dealing with gaussian kernels so k(x, y) = e ^ ((x - y) ** 2 / 2 * l ** 2)\n",
    "    \n",
    "    # gaussian kernel hyperparameters - adjusts the distance between points and variance\n",
    "    scale_kernel = 1\n",
    "    \n",
    "    x1_matrix = np.tile(x1, len(x2)).reshape((len(x2), len(x1)))\n",
    "    x2_matrix = np.tile(x2, len(x1)).reshape((len(x1), len(x2))).transpose()\n",
    "    \n",
    "    k_matrix = np.exp(-(x1_matrix - x2_matrix) ** 2 / (2 * l * l)) * scale_kernel ** 2\n",
    "    \n",
    "    return k_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(sample_x, sample_y, sigma_obs, l):\n",
    "    # The following function calculates the log-likelihood of observed data\n",
    "    # wrt to prior distribution for GP, i.e. zero mean and sigma given by k_gaussian(l) + sigma_obs ** 2 * I\n",
    "    \n",
    "    # Under that model the log-likelihood is given by \n",
    "    # -0.5 * y' * sigma(-1) * y - 0.5 * n * log(2pi) - 0.5 * log|sigma|\n",
    "    \n",
    "    # To make sense of the code below note that we express log-likelihood through the cholesky decomposition of sigma\n",
    "    # Then |sigma| = |chol| ** 2 (det of product is product of det)\n",
    "    # |chol| = prod(chol_ii) (because cholesky matrix is lower triangular)\n",
    "    # Thus, 0.5 * log|sigma| = sum(log(chol_ii))\n",
    "    \n",
    "    sigma = k_gaussian(sample_x, sample_x, l) + np.eye(len(sample_x)) * sigma_obs ** 2\n",
    "    chol = np.linalg.cholesky(sigma)\n",
    "    \n",
    "    # Calculating alpha = sigma(-1) * y (or solution to sigma * alpha = y) using cholesky matrix\n",
    "    # (This trick is taken from sklearn implementation of GP)\n",
    "    alpha = sp.linalg.cho_solve((chol, True), sample_y).reshape((-1, 1))\n",
    "    \n",
    "    log_lik = -0.5 * np.dot(sample_y.reshape(1, -1), alpha)\n",
    "    log_lik -= 0.5 * len(sample_x) * np.log(2 * np.pi)\n",
    "    log_lik -= np.trace(np.log(np.absolute(chol)))\n",
    "    \n",
    "    return log_lik[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_posterior(sample_x, sample_y, x, sigma_obs):\n",
    "    # Calculating posterior for gaussian processes\n",
    "    # I am specifically interested in posterior mean, std and cholesky matrix for postrior at sampled points (for nei)\n",
    "    # it is assumed that observations have some additional gaussian noise\n",
    "    \n",
    "    # Important: the method cannot handle sigma_obs=0 if I want to predict for sample_x\n",
    "    # Mostly numerical issues: with zero noise matrix to invert may not be positive-semidefinite\n",
    "\n",
    "    # Picking the optimal kernel hyperparameter (bruteforcing maximum likelihood wrt to prior GP model)\n",
    "    l = max(np.exp(np.linspace(np.log(0.01), np.log(1), 100)), \n",
    "            key = lambda z: log_likelihood(sample_x, sample_y, sigma_obs, z))\n",
    "#     print \"Picked kernel l: \", l\n",
    "    \n",
    "    # Separately calculating matrix used to calculate both mean and variance\n",
    "    K = np.dot(k_gaussian(sample_x, x, l),\n",
    "               np.linalg.inv(k_gaussian(sample_x, sample_x, l) + np.eye(len(sample_x)) * sigma_obs ** 2)\n",
    "              )\n",
    "    \n",
    "    mu = np.dot(K, sample_y)\n",
    "    sigma = k_gaussian(x, x, l) - np.dot(K, k_gaussian(x, sample_x, l))\n",
    "    std_1d = np.sqrt([sigma[i, i] for i in range(len(mu))])\n",
    "    \n",
    "    return mu.reshape(-1), std_1d.reshape(-1)\n",
    "\n",
    "def cholesky_posterior(sample_x, sample_y, sigma_obs):  \n",
    "    # Section to get the cholesky matrix\n",
    "    # Picking the optimal kernel hyperparameter (bruteforcing maximum likelihood wrt to prior GP model)\n",
    "    l = max(np.exp(np.linspace(np.log(0.01), np.log(1), 100)), \n",
    "            key = lambda z: log_likelihood(sample_x, sample_y, sigma_obs, z))\n",
    "    \n",
    "    # Calculating the posterior covariance matrix for observed data\n",
    "    K = np.dot(k_gaussian(sample_x, sample_x, l),\n",
    "               np.linalg.inv(k_gaussian(sample_x, sample_x, l) + np.eye(len(sample_x)) * sigma_obs ** 2)\n",
    "              )\n",
    "    sigma = k_gaussian(sample_x, sample_x, l) - np.dot(K, k_gaussian(sample_x, sample_x, l))\n",
    "    \n",
    "    # Because of numerical issues the supposed posterior covariance matrix may not be positive definite\n",
    "    # For that reason I add noise along main diagonal until the matrix is properly conditioned    \n",
    "    noise_addition = 0.00001\n",
    "    while True:\n",
    "        try:\n",
    "            chol = np.linalg.cholesky(sigma)\n",
    "            break\n",
    "        except:\n",
    "            print \"Problems with getting cholesky matrix, adding noise to main diagonal\"\n",
    "            sigma += noise_addition * np.eye(len(sample_x))\n",
    "    \n",
    "    return chol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below does basic EI steps - two options are to use skleran or to switch to custom implementation of GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_obs = 0.5\n",
    "\n",
    "pick_x = np.random.choice(range(len(x)), size=5, replace=False)\n",
    "sample_x = x[pick_x]\n",
    "sample_y = true_y[pick_x] + sigma_obs * np.random.randn(5)\n",
    "\n",
    "for t in range(1, 100):\n",
    "    \n",
    "    # Evaluating GP posterior\n",
    "    mu, std_1d = gp_posterior(sample_x, sample_y, x, sigma_obs)\n",
    "    \n",
    "    z = (mu - np.max(mu)) / std_1d\n",
    "    ei = std_1d * scipy.stats.norm.pdf(z) + z * std_1d * scipy.stats.norm.cdf(z)\n",
    "\n",
    "    pick_x = np.argmax(ei)\n",
    "    sample_x = np.append(sample_x, x[pick_x])\n",
    "    sample_y = np.append(sample_y, true_y[pick_x] + sigma_obs * np.random.randn(1))\n",
    "    \n",
    "    log_lik = log_likelihood(sample_x, sample_y, sigma_obs, 0.1)\n",
    "    \n",
    "    plt.plot(x, true_y, label=\"true_signal\")\n",
    "    plt.plot(sample_x, sample_y, \".\", color=\"r\", label=\"picked_x\")\n",
    "    plt.plot([sample_x[-1]], [sample_y[-1]], \".\", color=\"b\", label=\"last_x\")\n",
    "    plt.plot(x, mu, color=\"g\", label=\"posterior\")\n",
    "    plt.fill_between(x, mu - 2 * std_1d, mu + 2 * std_1d, color=\"g\", alpha=0.5)\n",
    "\n",
    "    plt.title(\"True and recovered signals\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.show()\n",
    "\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EI with noise correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_obs = 0.5\n",
    "n_mc = 10\n",
    "\n",
    "pick_x = np.random.choice(range(len(x)), size=1, replace=False)\n",
    "sample_x = x[pick_x]\n",
    "sample_y = true_y[pick_x] + sigma_obs * np.random.randn(1)\n",
    "\n",
    "for t in range(1, 100):\n",
    "    \n",
    "    # First step: figuring out posterior distributions for sampled points\n",
    "    sample_mu, _ = gp_posterior(sample_x, sample_y, sample_x, sigma_obs)   \n",
    "    chol = cholesky_posterior(sample_x, sample_y, sigma_obs) \n",
    "    nei = np.zeros((len(x),))\n",
    "    \n",
    "    # Second step: sampling from posterior distribution and calculating EI as if the sample has no noise\n",
    "    for n in range(n_mc):\n",
    "        # Not going for inverse of normal cdf as for now I an fine with usual MC (compared to QMC)\n",
    "        fresh_y = np.dot(chol, np.random.randn(len(sample_x)).reshape(-1, 1)) + sample_mu.reshape(-1, 1)\n",
    "\n",
    "        # Doing GP posterior as if sampled y are true noiseless values \n",
    "        # (noise is not zero so that the code would not crash)\n",
    "        mu, std_1d = gp_posterior(sample_x, fresh_y, x, 0.001)\n",
    "        \n",
    "        z = (mu - np.max(mu)) / std_1d\n",
    "        nei += std_1d * scipy.stats.norm.pdf(z) + z * std_1d * scipy.stats.norm.cdf(z)\n",
    "    \n",
    "    pick_x = np.argmax(nei)\n",
    "    sample_x = np.append(sample_x, x[pick_x])\n",
    "    sample_y = np.append(sample_y, true_y[pick_x] + sigma_obs * np.random.randn(1))\n",
    "\n",
    "    # Recalculating posterior for plotting\n",
    "    mu, std_1d = gp_posterior(sample_x, sample_y, x, sigma_obs)\n",
    "    \n",
    "    plt.plot(x, true_y, label=\"true_signal\")\n",
    "    plt.plot(sample_x, sample_y, \".\", color=\"r\", label=\"picked_x\")\n",
    "    plt.plot([sample_x[-1]], [sample_y[-1]], \".\", color=\"b\", label=\"last_x\")\n",
    "    plt.plot(x, mu, color=\"g\", label=\"posterior\")\n",
    "    plt.fill_between(x, mu - 2 * std_1d, mu + 2 * std_1d, color=\"g\", alpha=0.5)\n",
    "\n",
    "    plt.title(\"True and recovered signals\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.show()\n",
    "\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn implementation of GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "def gp_posterior(sample_x, sample_y, x, sigma_obs):\n",
    "    # calculating posterior for gaussian processes\n",
    "    # This version makes use of sklearn package to do inference\n",
    "    \n",
    "    # Main difference so far: sklearn implementation actually estimates kernel hyperparameters through ML\n",
    "    # n_restarts specifies how many times ML is performed when starting from random points\n",
    "    # noise in observed data is handled via alpha (is specifies the value to add along the main diagonal when inverting kernel matrix)\n",
    "    gp = GaussianProcessRegressor(kernel=RBF(length_scale=0.1),\n",
    "                                  alpha=sigma_obs**2,\n",
    "                                  n_restarts_optimizer=100,\n",
    "                                  normalize_y=False\n",
    "                                 )\n",
    "    \n",
    "    gp.fit(sample_x.reshape(-1, 1), sample_y.reshape(-1, 1))\n",
    "\n",
    "    mu, std_1d = gp.predict(x.reshape(-1, 1), return_std=True)\n",
    "    \n",
    "    return mu.reshape(-1), std_1d.reshape(-1)#, gp.L_\n",
    "\n",
    "def cholesky_posterior(sample_x, sample_y, sigma_obs):   \n",
    "    # Section to get the cholesky matrix\n",
    "    gp = GaussianProcessRegressor(kernel=RBF(length_scale=0.1),\n",
    "                                  alpha=sigma_obs**2,\n",
    "                                  n_restarts_optimizer=100,\n",
    "                                  normalize_y=False\n",
    "                                 )\n",
    "    \n",
    "    gp.fit(sample_x.reshape(-1, 1), sample_y.reshape(-1, 1))\n",
    "\n",
    "    _, sigma = gp.predict(sample_x.reshape(-1, 1), return_cov=True)\n",
    "    \n",
    "    # Because of numerical issues the supposed posterior covariance matrix may not be positive definite\n",
    "    # For that reason I add noise along main diagonal until the matrix is properly conditioned\n",
    "    noise_addition = 0.01\n",
    "    while True:\n",
    "        try:\n",
    "            chol = np.linalg.cholesky(sigma)\n",
    "            break\n",
    "        except:\n",
    "            print \"Problems with getting cholesky matrix, adding noise to main diagonal\"\n",
    "            sigma += noise_addition * np.eye(len(sample_x))\n",
    "    \n",
    "    return chol"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
